{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    get_embeddings_response = openai_client.embeddings.create(model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT, input=text)\n",
    "    return get_embeddings_response.data[0].embedding\n",
    "\n",
    "# Initialize Azure search client\n",
    "AZURE_SEARCH_SERVICE = os.getenv(\"AZURE_SEARCH_SERVICE\")\n",
    "AZURE_SEARCH_ENDPOINT = f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\"\n",
    "AZURE_SEARCH_SERVICE_KEY = os.getenv(\"AZURE_SEARCH_SERVICE_KEY\")\n",
    "search_service_cred = AzureKeyCredential(AZURE_SEARCH_SERVICE_KEY)\n",
    "\n",
    "AZURE_SEARCH_FULL_INDEX = \"gptkbindex\"\n",
    "search_client = SearchClient(AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_FULL_INDEX, credential=search_service_cred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm kiếm với Semantic và Hybrid search\n",
    "\n",
    "The search call below does a **hybrid search**, performing both a full-text search and a vector search in parallel.\n",
    "It merges those results using Reciprocal Rank Fusion (RRF). \n",
    "Finally, it re-ranks the merged results using the AI Search semantic ranker, a re-ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước tiên chúng ta cần thiết lập semantic search cho index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SemanticConfiguration, SemanticPrioritizedFields, SemanticField, SemanticSearch\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=search_service_cred)\n",
    "semantic_config = SemanticConfiguration(\n",
    "                    name=\"default-semantic-config\",\n",
    "                    prioritized_fields=SemanticPrioritizedFields(\n",
    "                        content_fields=[SemanticField(field_name=\"content\")]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])     \n",
    "current_index = index_client.get_index(AZURE_SEARCH_FULL_INDEX)\n",
    "current_index.semantic_search = semantic_search\n",
    "index_client.create_or_update_index(current_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"máy tính bảng có khả năng chơi game\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(user_question):\n",
    "        user_question_vector = get_embedding(user_question)\n",
    "        r = search_client.search(\n",
    "                user_question,\n",
    "                top=5, \n",
    "                vector_queries=[\n",
    "                        VectorizedQuery(vector=user_question_vector, k_nearest_neighbors=50, fields=\"embedding\")],\n",
    "                query_type=\"semantic\",\n",
    "                semantic_configuration_name=\"default-semantic-config\")\n",
    "\n",
    "        sources = \"\\n\".join([f\"{doc['sourcefile']}: {doc['content']}\\n\" for doc in r])\n",
    "        return sources\n",
    "\n",
    "result = search(user_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gửi câu hỏi và thông tin sản phẩm tới LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "Assistant helps answer customer's questions about the available products. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below.\n",
    "Each source has a name followed by colon and the actual information, include the source name for each fact you use.\n",
    "Use square brackets to reference the source, for example [info1.txt].\n",
    "\"\"\"\n",
    "user_question = \"có máy in màu không?\"\n",
    "\n",
    "sources = search(user_question)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "    {\"role\": \"user\", \"content\": user_question + \"\\nSources: \" + sources}\n",
    "]\n",
    "# Now we can use the matches to generate a response\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    temperature=0.7,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Câu hỏi: \", user_question)\n",
    "print(\"Bot: \", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cải thiện để xử lý conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(messages, question):\n",
    "    clone_messages = list(messages)[1:]\n",
    "    clone_messages.append({\"role\": \"user\", \n",
    "                           \"content\": f\"Đây là câu hỏi của user: {question}. Dựa trên lịch sử và câu hỏi hiện tại, tạo query tối đa 10 từ để tìm kiếm. Đừng đưa ra gợi ý ngoài câu hỏi.\"})\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        temperature=0,\n",
    "        messages=clone_messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "Assistant helps answer customer's questions about the available products. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below.\n",
    "Each source has a name followed by colon and the actual information, include the source name for each fact you use.\n",
    "Use square brackets to reference the source, for example [info1.txt].\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
    "\n",
    "while(True):\n",
    "    sleep(1)\n",
    "    question = input()\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "    \n",
    "    print(\"Human > \" + question)\n",
    "    query = generate_query(messages=messages, question=question)\n",
    "    print(f\"\\tGenerated query: {query}\")\n",
    "\n",
    "    sources = search(query)\n",
    "    \n",
    "    USER_MESSAGE = question + \"\\nSources: \" + sources\n",
    "    messages.append( {\"role\": \"user\", \"content\": USER_MESSAGE})\n",
    "    \n",
    "    # Now we can use the matches to generate a response\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    print(\"Bot > \" + answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_design_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
